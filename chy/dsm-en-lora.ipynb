{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d60fef6-e4b7-4d2b-aa7c-827e92474768",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79ac68fe-5a45-45e3-b505-c4f47c759fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template, standardize_sharegpt, train_on_responses_only\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "from peft import LoraConfig, TaskType\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d9de73-da67-4340-bfb6-94f465503118",
   "metadata": {},
   "source": [
    "# check unsloth ver issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef89d673-0171-46f1-b336-e89f6fbc54cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025.7.1\n",
      "2025.7.1\n",
      "0.19.1\n"
     ]
    }
   ],
   "source": [
    "# 7.2Ïùº Î¶¥Î¶¨Ï¶àÎ∂ÄÌÑ∞ Ïù¥Ïäà Î≥¥Í≥†Îê®\n",
    "# https://github.com/unslothai/unsloth/issues/3071\n",
    "# !pip install unsloth-zoo==2025.7.1 unsloth==2025.7.1\n",
    "# !pip install trl==0.19.1 \n",
    "import trl\n",
    "import unsloth\n",
    "import unsloth_zoo\n",
    "\n",
    "print(unsloth.__version__)\n",
    "print(unsloth_zoo.__version__)\n",
    "print(trl.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d664a19-710f-42bb-ba1f-68be85f58b39",
   "metadata": {},
   "source": [
    "# Î™®Îç∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00094814-44e7-4868-ab51-89e12af53531",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CARDS = ['unsloth/DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit', 'unsloth/Meta-Llama-3.1-8B-bnb-4bit']\n",
    "MODEL = MODEL_CARDS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af37602c-0f47-40de-a349-4f2e9a5c9d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.1: Fast Qwen2 patching. Transformers: 4.54.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.691 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7fcde5746245cba806d5d0d004c13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.7.1 patched 48 layers with 48 QKV layers, 48 O layers and 48 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL,\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = True,\n",
    "    dtype = None,\n",
    ")\n",
    "\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# model.enable_input_require_grads()\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64,\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
    "    use_rslora = True,\n",
    "    use_gradient_checkpointing = \"unsloth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f776b9-242c-46ef-bc31-fb587242f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_trainable_params(model, show_grad=False):\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    print(f\"ÌïôÏäµ Í∞ÄÎä•Ìïú ÌååÎùºÎØ∏ÌÑ∞ Îç©Ïñ¥Î¶¨: {len(trainable_params)}\")\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_num = sum(p.numel() for p in trainable_params)\n",
    "    print(f\"ÌïôÏäµ Í∞ÄÎä•Ìïú ÌååÎùºÎØ∏ÌÑ∞: {trainable_num} / Ï†ÑÏ≤¥ ÌååÎùºÎØ∏ÌÑ∞: {total_params} ({100*trainable_num/total_params:.4f}%)\")\n",
    "\n",
    "    if not show_grad:\n",
    "        return \n",
    "        \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            grad_status = param.grad is not None\n",
    "            grad_norm = param.grad.norm().item() if grad_status else None\n",
    "            print(f\"GRAD OK : {name:60}\")\n",
    "        else:\n",
    "            grad_status = param.grad is not None\n",
    "            grad_norm = param.grad.norm().item() if grad_status else None\n",
    "            print(f\"GRAD NO : {name:60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8777e4d1-8b70-4884-8f83-10f11a871e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÌïôÏäµ Í∞ÄÎä•Ìïú ÌååÎùºÎØ∏ÌÑ∞ Îç©Ïñ¥Î¶¨: 672\n",
      "ÌïôÏäµ Í∞ÄÎä•Ìïú ÌååÎùºÎØ∏ÌÑ∞: 275251200 / Ï†ÑÏ≤¥ ÌååÎùºÎØ∏ÌÑ∞: 9896776704 (2.7812%)\n"
     ]
    }
   ],
   "source": [
    "check_trainable_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59b97e-e672-4ab0-8a0c-a5f77250b5c0",
   "metadata": {},
   "source": [
    "# preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da4396d4-78f7-4aa8-8f7e-baf9dcc10a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(dialogue, summary=\"\", for_inference=False):\n",
    "    instruction = f\"\"\"Following the instructions below, summarize the given dialogue.\n",
    "---\n",
    "Instructions:\n",
    "1. Read the dialogue carefully.\n",
    "2. Make the summary concise and brief.\n",
    "3. Before generating the summary, predict the number of words your summary will contain and state it as \"Word count: X\".\n",
    "4. Write the summary to match the given target word count as closely as possible.\n",
    "5. When writing the summary, use as many words and expressions from the original dialogue as possible, rather than paraphrasing into new language.\n",
    "6. Preserve named entities in the summary.\n",
    "7. Use english and among special characters and symbols, only numbers, commas, and periods may be used.\n",
    "8. Reflect discourse relations, speech acts, and conversational intentions in the summary.\n",
    "---\n",
    "Dialogue:\n",
    "{dialogue}\n",
    "---\n",
    "\"\"\"\n",
    "    \n",
    "    gen_body = \"Word count:\" + SEP_TOKEN\n",
    "    if not for_inference:\n",
    "        n_words = len(summary.split(' '))\n",
    "        gen_body += f\"{n_words}\"\n",
    "    \n",
    "    if for_inference:\n",
    "        prompt = instruction + gen_body\n",
    "        return prompt, instruction\n",
    "        \n",
    "    gen_body += \"\\n---\\nSummary:\\n\"\n",
    "    prompt = instruction + gen_body + summary + EOS_TOKEN\n",
    "    # full_prompt = head_part + SEP_TOKEN + summary + EOS_TOKEN\n",
    "    \n",
    "    return prompt, instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4de9d25c-bc80-4f47-8abd-c18d109df1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer, show_prompt=None, for_inference=False):\n",
    "    inputs, labels = [], []\n",
    "    \n",
    "    for dialogue, summary in zip(examples[\"dialogue\"], examples[\"summary\"]):\n",
    "        full_prompt, inst_part = make_prompt(dialogue, summary, for_inference=for_inference)\n",
    "\n",
    "        if show_prompt:\n",
    "            print(full_prompt)\n",
    "            print(\"=\"*80)\n",
    "        \n",
    "        tokenized = tokenizer(full_prompt, max_length=2048, padding=\"max_length\", truncation=True)\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        label_ids = input_ids.copy()\n",
    "\n",
    "        # ÏûÖÎ†• Î∂ÄÎ∂Ñ ÌÜ†ÌÅ∞ÏùÑ Ï∞æÍ∏∞ ÏúÑÌï¥ instruction part ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï Í∏∏Ïù¥ ÌååÏïÖ\n",
    "        head_part_ids = tokenizer(inst_part, add_special_tokens=False)[\"input_ids\"]\n",
    "        head_part_len = len(head_part_ids)\n",
    "\n",
    "        # labelsÏóêÏÑú ÏûÖÎ†•Î∂ÄÎ∂ÑÏùÄ -100 ÏúºÎ°ú ÎßàÏä§ÌÇπ Ìï¥ loss Í≥ÑÏÇ∞ Ï†úÏô∏\n",
    "        for i in range(head_part_len):\n",
    "            label_ids[i] = -100\n",
    "\n",
    "        inputs.append(input_ids)\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    batch = {\n",
    "        \"input_ids\": inputs,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": [tokenized[\"attention_mask\"] for _ in range(len(inputs))],\n",
    "    }\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bf8351-47e3-4a2f-b4d4-a6fbd250f476",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d186247e-62ea-46d3-9cd7-890666ef6c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(151666, 5120, padding_idx=151654)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.eos_token)\n",
    "SEP_TOKEN = \"<sep>\"\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({\"sep_token\": SEP_TOKEN})\n",
    "# tokenizer.add_special_tokens({\"sep_token\": SEP_TOKEN, \"eos_token\": EOS_TOKEN})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a55aeea-2c94-4d7b-9a30-2da877559033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÌïôÏäµ ÏÉòÌîåÏàò (623, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/data/ephemeral/home/ds/origin/dsm_en_train.csv\")\n",
    "\n",
    "# df_train = df\n",
    "df_train = df.sample(frac=0.050, random_state=142)\n",
    "# df_evals = df.sample(frac=0.001, random_state=142)  # valid ÏÖã ÏòÅÎ¨∏Ìôî Î™ª ÌïòÏòÄÏùå... Í∑∏ÎÉ• train ÏùºÎ∂Ä ÏîÄ\n",
    "\n",
    "print('ÌïôÏäµ ÏÉòÌîåÏàò', df_train.shape)\n",
    "# print('ÌèâÍ∞Ä ÏÉòÌîåÏàò', df_evals.shape)\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "# dataset_valid = Dataset.from_pandas(df_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "45bc0f3a-82bb-4598-9342-91f978b24cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0a16e1dad448eea7a761f82c5999e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/623 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ÏÇ¥Ìé¥Î≥¥Î†§Î©¥ ÏÉòÌîåÏàò ÎÇÆÏ∂îÍ≥† show_prompt\n",
    "p_ds_train = dataset_train.map(lambda x: preprocess_function(x, tokenizer, show_prompt=False), \n",
    "                               batched=True, remove_columns=dataset_train.column_names)\n",
    "# p_ds_valid = dataset_valid.map(lambda x: preprocess_function(x, tokenizer, show_prompt=False, for_inference=True),  \n",
    "#                                batched=True, remove_columns=dataset_valid.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c50be3a4-3e3c-4efd-9aa4-4ae50bd93236",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f728e4-b838-4efa-8aa7-bad8f456b110",
   "metadata": {},
   "source": [
    "# metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "838c2d91-48a0-49dd-8076-201c9eade452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge_metrics(pred_results):\n",
    "    preds, labels = pred_results\n",
    "\n",
    "    preds[preds == -100] = tokenizer.pad_token_id\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "    \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # decoded_preds = tokenizer.batch_decode(preds, clean_up_tokenization_spaces=True)\n",
    "    \n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge1_list, rouge2_list, rougel_list = [], [], []\n",
    "\n",
    "    for pred, ref in zip(decoded_preds, decoded_labels):\n",
    "        scores = scorer.score(ref, pred)\n",
    "        rouge1_list.append(scores['rouge1'].fmeasure)\n",
    "        rouge2_list.append(scores['rouge2'].fmeasure)\n",
    "        rougel_list.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    avg_rouge1 = sum(rouge1_list)/len(rouge1_list)\n",
    "    avg_rouge2 = sum(rouge2_list)/len(rouge2_list)\n",
    "    avg_rougel = sum(rougel_list)/len(rougel_list)\n",
    "    final_score = avg_rouge1 + avg_rouge2 + avg_rougel\n",
    "    \n",
    "    return {\"rouge1\": avg_rouge1, \"rouge2\": avg_rouge2, \"rougel\": avg_rougel, \"final_score\": final_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4fba252-250c-4176-b926-1c10d4f2160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience = 3,\n",
    "    early_stopping_threshold = 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5379f03-744b-4ece-a875-37a75117cd5b",
   "metadata": {},
   "source": [
    "# trainer config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16792fb6-b72f-4026-81e1-62558ab17ba4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./{MODEL}-lora\",\n",
    "    report_to=\"none\",\n",
    "    logging_steps=4,\n",
    "    \n",
    "    save_strategy=\"steps\",\n",
    "    # eval_strategy='steps',\n",
    "    # eval_steps=4,\n",
    "    save_steps=20,\n",
    "    save_total_limit=1,    \n",
    "    # load_best_model_at_end=True,\n",
    "\n",
    "    bf16=True,   # fp16 GPU Î™®Îç∏ÎßàÎã§ Î∂àÏïàÏ†ïÏÑ± ÏûàÏùå\n",
    "    learning_rate=5e-5,\n",
    "    greater_is_better = False,\n",
    "    \n",
    "    # per_device_eval_batch_size=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=2,\n",
    "\n",
    "    optim=\"adamw_8bit\",  # 8bit ÏòµÌã∞ÎßàÏù¥Ï†∏ÎèÑ Î∂àÏïàÏ†ïÏÑ± ÏûàÏùå\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = p_ds_train,\n",
    "    # eval_dataset = p_ds_valid,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    # compute_metrics = lambda pred: compute_metrics(pred, tokenizer, config),\n",
    "    args = training_args)\n",
    "\n",
    "# trainer.add_callback(early_stopping_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13808203-e4cd-4852-b744-6a1773cda122",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42810530-1e8d-4f90-a7a9-4b38bca1f72e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ec9c5a-8d8a-4e67-832d-d3160d6d03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('stop here')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2ee650-d35d-4df2-9ded-f2e4f59c767f",
   "metadata": {},
   "source": [
    "# save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f09399b-e075-4f12-ab03-e3735e091923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"./dlb_dialogue_summary\")\n",
    "# tokenizer.save_pretrained(\"./dlb_dialogue_summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5d6fac-35f6-4fc9-805e-33926b341a26",
   "metadata": {},
   "source": [
    "# resume train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef673343-866a-4d17-a6cd-5e9c9cfed785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(training_args.output_dir)\n",
    "# model.save_pretrained_merged(training_args.output_dir, tokenizer, save_method=\"merged_16bit\")\n",
    "# model.load_adapter(training_args.output_dir, adapter_name=\"demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1897f6-9253-4d4e-a38b-2ba0a5719a8e",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ff152951-6890-4489-b642-16522f0f1463",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "\"\"\"#Person1#: Excuse me. Where is the exit from here?  \n",
    "#Person2#: Actually, I‚Äôm trying to find it myself right now.  \n",
    "#Person1#: This place is really confusing, isn‚Äôt it?  \n",
    "#Person2#: Absolutely. Oh, do you see that sign over there?  \n",
    "#Person1#: That must be the exit.  \n",
    "#Person2#: Great. Let‚Äôs go check it out.\n",
    "\"\"\", \n",
    "\"\"\"#Person1#: Who is that person over there?  \n",
    "#Person2#: They‚Äôre the new teacher.  \n",
    "#Person1#: What‚Äôs the teacher like?  \n",
    "#Person2#: They‚Äôre really kind.  \n",
    "#Person1#: What subject do they teach?  \n",
    "#Person2#: English, of course.  \n",
    "#Person1#: Are they your teacher?  \n",
    "#Person2#: Yeah, they teach our class three times a week.  \n",
    "#Person1#: Can they speak Chinese to students?  \n",
    "#Person2#: Not very well.  \n",
    "#Person1#: It‚Äôs good that you know English!\"\"\",\n",
    "\"\"\"#Person1#: Excuse me, you can‚Äôt park here.  \n",
    "#Person2#: I‚Äôm waiting for my friends. I‚Äôll move it right away.  \n",
    "#Person1#: This is a no-parking zone.  \n",
    "#Person2#: But I didn‚Äôt see any sign.  \n",
    "#Person1#: There‚Äôs a no-parking sign at the corner of the road. You must have seen it when you entered.  \n",
    "#Person2#: I didn‚Äôt see it.  \n",
    "#Person1#: I‚Äôm sorry, but I have to issue a ticket. You can file an objection with the court within 14 days. If you can prove you‚Äôre not at fault, you may avoid the fine.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ba028817-6b66-49d4-9288-24e32f7a5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, _ = make_prompt(samples[2], for_inference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "25d32c2c-ae10-425f-af04-393843b2a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "95c6e96a-b0fd-411c-9f49-70b9c83abef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>Following the instructions below, summarize the given dialogue.\n",
      "---\n",
      "Instructions:\n",
      "1. Read the dialogue carefully.\n",
      "2. Make the summary concise and brief.\n",
      "3. Before generating the summary, predict the number of words your summary will contain and state it as \"Word count: X\".\n",
      "4. Write the summary to match the given target word count as closely as possible.\n",
      "5. When writing the summary, use as many words and expressions from the original dialogue as possible, rather than paraphrasing into new language.\n",
      "6. Preserve named entities in the summary.\n",
      "7. Use english and among special characters and symbols, only numbers, commas, and periods may be used.\n",
      "8. Reflect discourse relations, speech acts, and conversational intentions in the summary.\n",
      "---\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Excuse me, you can‚Äôt park here.  \n",
      "#Person2#: I‚Äôm waiting for my friends. I‚Äôll move it right away.  \n",
      "#Person1#: This is a no-parking zone.  \n",
      "#Person2#: But I didn‚Äôt see any sign.  \n",
      "#Person1#: There‚Äôs a no-parking sign at the corner of the road. You must have seen it when you entered.  \n",
      "#Person2#: I didn‚Äôt see it.  \n",
      "#Person1#: I‚Äôm sorry, but I have to issue a ticket. You can file an objection with the court within 14 days. If you can prove you‚Äôre not at fault, you may avoid the fine.\n",
      "---\n",
      "Word count:<sep>19\n",
      "---\n",
      "Summary:\n",
      "#Person1# informs #Person2# that they have to issue a parking ticket because they parked in a no-parking zone.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.3)\n",
    "# outputs = model.generate(**inputs, max_new_tokens=23, temperature=0.8, top_k=50, top_p=0.95)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18201b86-0bd5-4ba7-b32c-160c4e716554",
   "metadata": {},
   "source": [
    "# inference trial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "048f719d-b05b-42da-9dcb-18c6849490ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trial = pd.read_csv('/data/ephemeral/home/ds/origin/dsm_trial_en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8d484f31-f2da-4cda-a938-d7e514c548f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dialogue, max_new_tokens, temp=0.3, top_p=1.00, num_return=1, show_gen=False, skip_sp_tok=False):\n",
    "    # skip_sp_tokÏóê Îî∞Îùº pos Í∞í ÏÇ¨Ïö© Î∂àÍ∞ÄÌï† Ïàò ÏûàÏùå\n",
    "    prompt, inst_part = make_prompt(dialogue, for_inference=True)\n",
    "\n",
    "    pos_word_count = len(inst_part)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    outputs = model.generate(**inputs, \n",
    "                             max_new_tokens=max_new_tokens, \n",
    "                             temperature=temp, \n",
    "                             top_p=top_p, \n",
    "                             num_return_sequences=num_return,\n",
    "                             do_sample=True)\n",
    "    \n",
    "    gens = tokenizer.decode(outputs[0], skip_special_tokens=skip_sp_tok)\n",
    "    if show_gen:\n",
    "        print(gens)\n",
    "\n",
    "    summary = \"\"\n",
    "    pos_summary = gens.find('Summary:\\n')\n",
    "    if pos_summary > 0:\n",
    "        summary = gens[pos_summary:]\n",
    "\n",
    "    return gens, summary, prompt, pos_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5b75f6d5-f50c-410b-b13b-56b06ec50ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gens, summary, prompt, pos_word = inference(df_trial.iloc[2]['dialogue'], max_new_tokens=60, skip_sp_tok=True, temp=0.8, top_p=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "778cef5f-d443-457b-8cd1-957a4a3c19d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(text[741:])\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4420a538-a623-4a02-9d3a-124bb71c50f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4054817b-253c-482a-bafd-b899fec882af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c22d2c25af4facb65814322e8546ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df = df_trial.iloc[0:10]\n",
    "df = df_trial\n",
    "\n",
    "outs = []\n",
    "for i, ex in tqdm(df.iterrows(), total=len(df)):\n",
    "    fname, dialogue = ex['fname'], ex['dialogue']\n",
    "    # TODO: dialogueÏóê ÎπÑÎ°ÄÌïòÎäî summary Í∏∏Ïù¥ ÏöîÏ≤≠ prompt Ï∂îÍ∞Ä\n",
    "    gens, summary, _, _ = inference(dialogue, max_new_tokens=60, skip_sp_tok=True, temp=0.8, top_p=0.8)\n",
    "    outs.append((fname, gens, summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9d603ea5-9b2d-415c-a0b2-1924efc3950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(outs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
